seed: ${..seed}
algo: PPO

print_every: 1
ckpt_every: 100

metrics_kwargs:
  save_video_every: 0
  save_video_consecutive: 0
  tracker_len: 100

normalize_input: True

network:
  actor_critic: ActorCritic
  actor_critic_kwargs:
    separate_value_mlp: True
    mlp_kwargs:
      units: [32, 32]

      norm_type: null
      act_type: ELU
    actor_dist_kwargs:
      dist: normal
    fixed_sigma: True

ppo:
  multi_gpu: ${...multi_gpu}
  num_actors: ${...task.env.numEnvs}
  normalize_value: True
  value_bootstrap: True
  reward_shaper:
    fn: scale
    scale: 0.1
  clip_value_loss: False
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  optim_type: Adam
  optim_kwargs:
    lr: 3e-4
    eps: 1e-5
  lr_schedule: kl  # 'fixed' | 'linear' | 'kl' | 'cos'
  kl_threshold: 0.008
  entropy_coef: 0.0
  e_clip: 0.2
  use_smooth_clamp: False
  critic_coef: 4
  bounds_loss_coef: 0.0001
  bounds_type: bound
  max_grad_norm: 1.0
  truncate_grads: True
  horizon_length: 16
  minibatch_size: 8192
  mini_epochs: 8
  max_agent_steps: 1.5e6
