seed: ${..seed}
algo: OTIL

print_every: 1
ckpt_every: 100

network:
  normalize_input: True
  encoder: null
  encoder_kwargs: null

  actor: Actor
  actor_kwargs:
    weight_init: orthogonalg1
    mlp_kwargs:
      units: [128, 64, 32]
      norm_type: LayerNorm
      act_type: SiLU
    dist_kwargs: { dist_type: squashed_normal, minlogstd: -5.0, maxlogstd: 2.0 }
    fixed_sigma: False
  tanh_clamp: False

otil:
  multi_gpu: ${...multi_gpu}
  num_actors: ${...task.env.numEnvs}
  demos_path: ""

  reward_shaper:
    fn: scale
    scale: 1.0

  max_epochs: 2000
  max_agent_steps: 4.1e6
  horizon_len: 32
  num_critic_batches: 4

  optim_type: AdamW
  actor_optim_kwargs: { lr: 2e-3, betas: [0.7, 0.95] }
  lr_schedule: linear # linear | constant
  max_grad_norm: 0.5
  truncate_grads: True
